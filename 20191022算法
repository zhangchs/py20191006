算法：
  有监督学习：既有特征值又有目标值
    分类：目标值离散
      k-近邻算法 sklearn.neighbors
        [(a1-b1)^2+(a2-b2)^2+(a3-b3)^2]^(1/2)
      贝叶斯分类 sklearn.naive_bayes
        概率基础
          联合概率:包含多个条件，同时成立P(A,B)
            P(A,B)=P(A)P(B)
          条件概率：事件A在另一个事件B已经发生条件下的概率P(A|B)
            P(A1,A2|B)=P(A1|B)P(A2|B)
        特征之间要独立（垂直）
        贝叶斯公式：P(C|W)=P(W|C)P(C)/P(W)
          P(C|F1,F2,...)=P(F1,F2,...|C)P(C)/P(F1,F2,...)
          w为给定文档的特征值，c为文档类别
          P(C):每个文档类别的概率（某文档类别数/总文档数量）
          P(W|C):给定类别下特征（被预测文档中出现的词）的概率
            计算方法：P(F1|C)=Ni/N   (训练文档中计算)
            Fi为该F1词在C类别所有文档中出现的次数
            N为所属类别C下的文档所有词出现的次数和
          P(F1,F2,...) 预测文档中每个词的频率
          为解决频率为0的问题引入拉普拉斯平滑系数
            P(F1|C)=(Ni+α)/(N+αm)   α一般设为1
      决策树与随机深林 sklearn.tree
        香农信息墒：H(X)=∑x∈XP(x)logP(x) 概率相同时公式成立
        信息增益：g(D,A)=H(D)-H(D|A)
      逻辑回归 sklearn.linear_model.LogisticRegression
        sigmoid g(z)=1/[1+(e^-z)]
      神经网络
    回归：目标值连续
      线性回归 sklearn.linear_model.LinearRegression（SGDRegression）
        f(x)=w1x1+w2x2+...+wdxd+b
        损失函数：∑[h(x)-y]^2(最小二乘法)
        求权重w:
          正规方程：w={[(X^T)X]^-1}(X^T)y
          梯度下降：均方误差[∑(yi-y)^2]/m
        性能评估：
      岭回归 sklearn.linear_model.Ridge
    标注：
      隐马尔可夫模型
  无监督学习：只有特征值
    聚类
      k-means
        评估标准：轮廓系数
      
100个样本，30个患者。检测出25个患者，其中5个误判。
  准确率：85/100
  精确率：20/25
  召回率：20/30
    

    
    
